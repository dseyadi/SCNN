{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random, tensorflow, tensorflow.keras, numpy as np, matplotlib.pyplot as plt\n",
    "import cv2, uuid \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create positive / negative / anchor folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the paths\n",
    "pos_path= os.path.join('dataset', 'positive')\n",
    "neg_path= os.path.join('dataset', 'negative')\n",
    "anc_path= os.path.join('dataset', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directories\n",
    "if not os.path.exists(pos_path):\n",
    "    os.makedirs(pos_path)\n",
    "  \n",
    "if not os.path.exists(neg_path):  \n",
    "    os.makedirs(neg_path)\n",
    "    \n",
    "if not os.path.exists(anc_path):\n",
    "    os.makedirs(anc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move the images from the lfw-deepfunneled directory to the neg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset', 'lfw-deepfunneled', 'Siames-facialVerification.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orginal=os.getcwd()\n",
    "os.listdir(orginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir('lfw-deepfunneled'):\n",
    "    for file in os.listdir(os.path.join('lfw-deepfunneled', directory)):\n",
    "        ex_path = os.path.join('lfw-deepfunneled', directory, file)\n",
    "        new_path = os.path.join(neg_path, file)\n",
    "        os.replace(ex_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Positive and Anchor classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture every frame\n",
    "\n",
    "# connect to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# flag to start/stop capturing frames\n",
    "start_capturing = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # cut the frame\n",
    "    frame = frame[170:170+250, 230:230+250, :]\n",
    "\n",
    "    # check for 's' key press\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        start_capturing = not start_capturing\n",
    "\n",
    "    # collect anchor images if 's' has been pressed\n",
    "    if start_capturing:\n",
    "        imgName = os.path.join(anc_path, '{}.jpg'.format(uuid.uuid1())) # to create a unique image name\n",
    "        cv2.imwrite(imgName, frame) # save the anchor img\n",
    "\n",
    "    cv2.imshow('image collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'): # to close the frame when 'q' is clicked\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_file will grap all images \n",
    "# these will search for every jpg file in the path\n",
    "anchor = tensorflow.data.Dataset.list_files(anc_path+'\\*.jpg').take(300) \n",
    "positive = tensorflow.data.Dataset.list_files(pos_path+'\\*.jpg').take(300)\n",
    "negative = tensorflow.data.Dataset.list_files(neg_path+'\\*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'dataset\\\\anchor\\\\e523be94-d260-11ee-96d0-920f0c4673c7.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_test = anchor.as_numpy_iterator()\n",
    "dir_test.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize and scale the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    load_img= tensorflow.io.read_file(file_path) #read the img\n",
    "    image = tensorflow.io.decode_jpeg(load_img) #load the img\n",
    "    image = tensorflow.image.resize( image, (105,105)) #resize the img\n",
    "    image = image / 255.0 #scale the img\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13000,), dtype=float32, numpy=array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anchor + positive => 1,1,1,1,1\n",
    "tensorflow.ones(len(anchor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13000,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anchor + negative => 0,0,0,0,0\n",
    "tensorflow.zeros(len(anchor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset.zip will zip its arguments together so i can iterate over them \n",
    "#from_tensor_slices is a data loader \n",
    "\n",
    "positives = tensorflow.data.Dataset.zip((anchor, positive, tensorflow.data.Dataset.from_tensor_slices(tensorflow.ones(len(anchor))))) \n",
    "negatives = tensorflow.data.Dataset.zip((anchor, negative, tensorflow.data.Dataset.from_tensor_slices(tensorflow.zeros(len(anchor))))) \n",
    "\n",
    "data = positives.concatenate(negatives) #to join the postives and negatives sample together in one big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset shapes: ((), (), ()), types: (tf.string, tf.string, tf.float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample= data.as_numpy_iterator()\n",
    "example1= sample.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess the twin images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, val_img, label):\n",
    "    return (preprocess(input_img), preprocess(val_img)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = preprocess_twin(*example1) #the * will unpack the example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(check[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(check[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build dataloader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin) #map it to apply preprocess_twin to all of the data\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=12000) #shuffle it to a buffer of 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (((105, 105, None), (105, 105, None)), ()), types: ((tf.float32, tf.float32), tf.float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2= data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample2.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2= sample2.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example2[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example2[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18200"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(data)*.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 105, 105, None), (None, 105, 105, None)), (None,)), types: ((tf.float32, tf.float32), tf.float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data #the first None refere to the number of images in the batch (16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(data)*.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "val_data = data.skip(round(len(data)*.7)) #skip 70% for training\n",
    "val_data = val_data.take(round(len(data)*.2)) #take 20% for validation\n",
    "val_data = val_data.batch(16)\n",
    "val_data = val_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(data)*.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_data = data.skip(round(len(data)*.7)) #skip 70% for training\n",
    "test_data = test_data.skip(round(len(data)*.2)) #skip 20% for validation\n",
    "test_data = test_data.take(round(len(data)*.1)) #take 10% for testing\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 105, 105, None), (None, 105, 105, None)), (None,)), types: ((tf.float32, tf.float32), tf.float32)>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buliding the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer():\n",
    "    #the image to be embedded\n",
    "    inp_img = Input(shape=(105,105,3), name='input_image')\n",
    "\n",
    "    conv1 = Conv2D(64, (10,10), activation='relu')(inp_img)\n",
    "    maxP1 = MaxPooling2D(64, (2,2), padding='same')(conv1)\n",
    "\n",
    "    conv2= Conv2D(128, (7,7), activation= 'relu')(maxP1)\n",
    "    maxP2 = MaxPooling2D(64, (2,2), padding='same')(conv2)\n",
    "\n",
    "    conv3= Conv2D(128, (4,4), activation='relu')(maxP2)\n",
    "    maxP3= MaxPooling2D(64, (2,2), padding='same')(conv3)\n",
    "\n",
    "\n",
    "    conv4= Conv2D(256, (4,4), activation='relu')(maxP3)\n",
    "    flatten = Flatten()(conv4)\n",
    "    dense = Dense(4096, activation='sigmoid')(flatten)\n",
    "\n",
    "    return Model(inputs=[inp_img], outputs=[dense], name=\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the distance layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distance_layer(Layer): #declaring Distance_layer class which inherits from the Layer clas\n",
    "    \n",
    "    # i sat (**kwargs) to accept any parameter\n",
    "    def __init__(self, **kwargs):\n",
    "        # i sat it to super to call the Layer class, to ensures that Distance_layer is properly initialized as a Layer\n",
    "        super().__init__()\n",
    "       \n",
    "    # calculate the similarity of two images\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tensorflow.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Distance_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Distance_layer at 0x2e0f63d9be0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model():\n",
    "    #the input image\n",
    "    anchor_image = Input(name='anchor_image', shape=(105,105,3))\n",
    "\n",
    "    #the validation image (positive OR negative)\n",
    "    validation_image= Input(name='validation_img', shape=(105,105,3))\n",
    "\n",
    "    #get the distance between the anchor image embedding and the validation image embedding\n",
    "    dist_layer = Distance_layer()\n",
    "    dist_layer._name = 'distance'\n",
    "    distances = dist_layer(embedding(anchor_image), embedding(validation_image))\n",
    "    \n",
    "    #classify the input as valid or not\n",
    "    classify = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[anchor_image, validation_image], outputs=classify, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_image (InputLayer)       [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "validation_img (InputLayer)     [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Functional)          (None, 4096)         38960448    anchor_image[0][0]               \n",
      "                                                                 validation_img[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "distance (Distance_layer)       (None, 4096)         0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            4097        distance[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "siamese_model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "history = siamese_model.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = siamese_model.evaluate(test_data)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss= history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(accuracy)+1)\n",
    "\n",
    "plt.plot(epochs, accuracy, 'r', label='training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='val accuracy')\n",
    "plt.title('training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='val loss')\n",
    "plt.title('training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = siamese_model.predict([test_input])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1 if prediction > 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = Precision()\n",
    "precision.update_state(y_true, y_hat)\n",
    "precision.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_input is a tuple of two lists of images\n",
    "first_image_pair = test_input[0]\n",
    "second_image_pair = test_input[1]\n",
    "\n",
    "# Select image from each pair\n",
    "first_image = first_image_pair[0]\n",
    "second_image = second_image_pair[0]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display the first image\n",
    "axs[0].imshow(first_image)\n",
    "axs[0].set_title('Label: ' + str(y_true[0]))\n",
    "\n",
    "# Display the second image\n",
    "axs[1].imshow(second_image)\n",
    "axs[1].set_title('Label: ' + str(y_true[0]))\n",
    "\n",
    "# Remove the x and y ticks\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save('siamesemodel600.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Distance_layer"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Distance_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow.keras.models.load_model('siamesemodel600.h5', \n",
    "                                   custom_objects={'Distance_layer':Distance_layer, 'RMSprop': tensorflow.keras.optimizers.RMSprop, 'BinaryCrossentropy':tensorflow.losses.BinaryCrossentropy}, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_image (InputLayer)       [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "validation_img (InputLayer)     [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Functional)          (None, 4096)         38960448    anchor_image[0][0]               \n",
      "                                                                 validation_img[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "distance_layer_2 (Distance_laye (None, 4096)         0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            4097        distance_layer_2[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the paths\n",
    "inp_path= os.path.join('realData', 'inputImage')\n",
    "ver_path= os.path.join('realData', 'verifyImage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realData\\\\inputImage'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directories\n",
    "if not os.path.exists(inp_path):\n",
    "    os.makedirs(inp_path)\n",
    "\n",
    "if not os.path.exists(ver_path):\n",
    "    os.makedirs(ver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    #cut the frame, discription bellow\n",
    "    frame = frame[170:170+250, 230:230+250, :]\n",
    "\n",
    "    #collect verify images\n",
    "    if cv2.waitKey(1) & 0XFF == ord('v'): #to collect verifiy image click on 'v'\n",
    "        imgName = os.path.join(ver_path, '{}.jpg'.format(uuid.uuid1())) #to create a unique image name\n",
    "        cv2.imwrite(imgName, frame) #save the anchor img\n",
    "\n",
    "    cv2.imshow('image collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'): #to close the fram when i click 'q'\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(ver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the verification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification(model, detection_threshold, ver_threshold):\n",
    "    results = []\n",
    "    for image in os.listdir(ver_path):\n",
    "        input_image_path = os.path.join('realData', 'inputImage', 'inputImage.jpg')\n",
    "        ver_image_path = os.path.join('realData', 'verifyImage', image)\n",
    "        \n",
    "        # Preprocess input and verification images\n",
    "        input_img = preprocess(input_image_path)\n",
    "        ver_img = preprocess(ver_image_path)\n",
    "\n",
    "        # Make Predictions \n",
    "        result = model.predict(list(np.expand_dims([input_img, ver_img], axis=1))) #wrap it into one array because i want one sample \n",
    "        results.append(result) #the results will be in one big array\n",
    "    \n",
    "    # Detection Threshold is the matrix of which a prediciton is considered positive \n",
    "    detectionThreshold = np.sum(np.array(results) > detection_threshold) #how many of the results are a match\n",
    "    \n",
    "    # Verify Threshold is the positive predictions / all positive sample\n",
    "    verificationThreshold = detectionThreshold / len(os.listdir(ver_path))\n",
    "\n",
    "    # check if the user is verify or not (TRUE / FALSE)\n",
    "    verified = verificationThreshold > ver_threshold\n",
    "\n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [array([[0.5403813]], dtype=float32), array([[0.26774818]], dtype=float32), array([[0.26774818]], dtype=float32), array([[0.27946565]], dtype=float32), array([[0.4498254]], dtype=float32), array([[0.5433409]], dtype=float32), array([[0.73319113]], dtype=float32), array([[0.9261545]], dtype=float32), array([[0.96841496]], dtype=float32), array([[0.99098]], dtype=float32), array([[0.9863265]], dtype=float32), array([[0.995268]], dtype=float32), array([[0.11137912]], dtype=float32), array([[0.38676274]], dtype=float32), array([[0.9965515]], dtype=float32), array([[0.99954706]], dtype=float32), array([[0.99940175]], dtype=float32), array([[0.9967697]], dtype=float32), array([[0.0675973]], dtype=float32), array([[0.26572478]], dtype=float32), array([[0.39905134]], dtype=float32), array([[0.6652823]], dtype=float32), array([[0.8194928]], dtype=float32), array([[0.9408945]], dtype=float32), array([[0.49062756]], dtype=float32), array([[0.996929]], dtype=float32), array([[0.99987763]], dtype=float32), array([[0.99606216]], dtype=float32), array([[0.9943975]], dtype=float32), array([[0.99598396]], dtype=float32), array([[0.9968039]], dtype=float32), array([[0.99931765]], dtype=float32), array([[0.98216444]], dtype=float32), array([[0.99984926]], dtype=float32), array([[0.98820543]], dtype=float32), array([[0.76727486]], dtype=float32), array([[0.76431036]], dtype=float32), array([[0.8431608]], dtype=float32), array([[0.7621397]], dtype=float32), array([[0.7034562]], dtype=float32), array([[0.09563181]], dtype=float32), array([[0.88197935]], dtype=float32), array([[0.10140347]], dtype=float32), array([[0.9671873]], dtype=float32), array([[0.9671873]], dtype=float32), array([[0.98654133]], dtype=float32), array([[0.17334747]], dtype=float32), array([[0.98650455]], dtype=float32), array([[0.99445915]], dtype=float32), array([[0.247266]], dtype=float32)]\n",
      "Verified: False\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "detection_threshold = 0.8\n",
    "verification_threshold = 0.5\n",
    "results, verified = verification(model, detection_threshold, verification_threshold)\n",
    "print(f\"Results: {results}\")\n",
    "print(f\"Verified: {verified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Results: [0.0675973, 0.09563181, 0.101403475, 0.11137912, 0.17334747, 0.247266, 0.26572478, 0.26774818, 0.26774818, 0.27946565, 0.38676274, 0.39905134, 0.4498254, 0.49062756, 0.5403813, 0.5433409, 0.6652823, 0.7034562, 0.73319113, 0.7621397, 0.76431036, 0.76727486, 0.8194928, 0.8431608, 0.88197935, 0.9261545, 0.9408945, 0.9671873, 0.9671873, 0.96841496, 0.98216444, 0.9863265, 0.98650455, 0.98654133, 0.98820543, 0.99098, 0.9943975, 0.99445915, 0.995268, 0.99598396, 0.99606216, 0.9965515, 0.9967697, 0.9968039, 0.996929, 0.99931765, 0.99940175, 0.99954706, 0.99984926, 0.99987763]\n"
     ]
    }
   ],
   "source": [
    "# Extract the float values from each sub-array\n",
    "float_values = [item[0][0] for item in results]\n",
    "\n",
    "# Sort the float values in ascending order\n",
    "sorted_float_values = sorted(float_values)\n",
    "\n",
    "print(f\"Sorted Results: {sorted_float_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.squeeze(results) > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28/len(os.listdir(ver_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV real time verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# connect to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    #cut the frame, discription bellow\n",
    "    frame = frame[170:170+250, 230:230+250, :]\n",
    "    cv2.imshow('Verification', frame)\n",
    "\n",
    "    #verification trigger\n",
    "    if cv2.waitKey(1) & 0XFF == ord('v'): #to verify the user click on 'v'\n",
    "        cv2.imwrite(os.path.join('realData', 'inputImage', 'inputImage.jpg'), frame) #save the anchor img\n",
    "        # call the verify function\n",
    "        results, verified = verification(model, 0.9, 0.7) \n",
    "        print(verified)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'): #to close the fram when i click 'q'\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
